{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "K6vroe5RpnZ5"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio==5.29.0 pdfplumber==0.11.6 nltk==3.9.1 matplotlib==3.10.0 wordcloud==1.9.4 textstat==0.7.2 spacy==3.8.5 weasyprint==65.1 markdown2==2.5.3 python-docx==1.1.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords', quiet=False)\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K7P4CFSFxo6",
        "outputId": "7b5c6341-bc46-45a5-9421-c261b3805d79"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File Handling / Formats\n",
        "import pdfplumber\n",
        "from docx import Document\n",
        "from markdown2 import markdown\n",
        "from weasyprint import HTML\n",
        "import zipfile\n",
        "\n",
        "# Natural Language Processing\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
        "import spacy\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Text Feature Extraction / Analysis\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Utilities\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import logging"
      ],
      "metadata": {
        "id": "23i4UEvsvn6S"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocReader:\n",
        "    \"\"\"\n",
        "    A utility class to extract text content from supported document types.\n",
        "\n",
        "    Supported formats:\n",
        "    - PDF (.pdf)\n",
        "    - Word Document (.docx)\n",
        "    - Plain Text (.txt)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_path):\n",
        "        \"\"\"\n",
        "        Initialize the DocReader with the path to a document file.\n",
        "\n",
        "        Parameters:\n",
        "            file_path (str): The full path to the input document file.\n",
        "                             Supported formats: .pdf, .docx, or .txt\n",
        "        \"\"\"\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def extract_text(self):\n",
        "        \"\"\"\n",
        "        Extract the textual content from the provided document.\n",
        "\n",
        "        Returns:\n",
        "            str: The full extracted text from the document.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the file extension is not one of the supported types.\n",
        "        \"\"\"\n",
        "        if self.file_path.endswith('.pdf'):\n",
        "            with pdfplumber.open(self.file_path) as pdf:\n",
        "                text = \"\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
        "                return text\n",
        "        elif self.file_path.endswith('.docx'):\n",
        "            doc = Document(self.file_path)\n",
        "            return \" \".join(para.text for para in doc.paragraphs)\n",
        "        elif self.file_path.endswith('.txt'):\n",
        "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Use .pdf, .docx, or .txt.\")"
      ],
      "metadata": {
        "id": "YgeMAqUyq3Wq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordLevelAnalysis:\n",
        "    \"\"\"\n",
        "    A class to perform detailed word-level analysis on a given text.\n",
        "\n",
        "    This includes:\n",
        "    - Tokenization and POS tagging\n",
        "    - TF-IDF keyword extraction\n",
        "    - N-gram frequency analysis\n",
        "    - Co-occurrence of keywords\n",
        "    - Stop word ratio\n",
        "    - Lexical diversity\n",
        "    - Average word length\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text):\n",
        "        \"\"\"\n",
        "        Initialize the WordLevelAnalysis with input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The raw input text to be analyzed.\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        self.words = word_tokenize(self.text)\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def word_statstics(self):\n",
        "        \"\"\"\n",
        "        Perform a full suite of word-level analyses on the text.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing results from multiple analyses, including:\n",
        "                - filtered_words (list): Cleaned tokens without stopwords or punctuation.\n",
        "                - top_keywords (list): Top TF-IDF keywords.\n",
        "                - pos_tags (list): Top 5 most common part-of-speech tags.\n",
        "                - avg_word_length (float): Average word length in characters.\n",
        "                - stop_word_ratio (float): Ratio of stopwords to total words.\n",
        "                - bigrams (list): Top bigram frequencies.\n",
        "                - co_occurrences (list): Most common co-occurring keyword pairs.\n",
        "                - lexical_diversity (float): Type-Token Ratio of the text.\n",
        "        \"\"\"\n",
        "        filtered_words = [word for word in self.words if word.isalnum() and word not in self.stop_words]\n",
        "        top_keywords = self.tfidf_keywords()\n",
        "        tagged_words = nltk.pos_tag(self.words)\n",
        "        pos_tags = [tag for word, tag in tagged_words if word.isalnum()]\n",
        "        pos_freq = FreqDist(pos_tags)\n",
        "        top_pos = pos_freq.most_common(5)\n",
        "        avg_word_len = self.avg_word_length()\n",
        "        stop_word_ratio = self.stop_word_ratio()\n",
        "        bigrams = self.ngram_analysis(n=2)\n",
        "        co_occurrences = self.keyword_co_occurrence()\n",
        "        ttr = self.lexical_diversity()\n",
        "\n",
        "        return {\n",
        "            \"filtered_words\": filtered_words,\n",
        "            \"top_keywords\": top_keywords,\n",
        "            \"pos_tags\": top_pos,\n",
        "            \"avg_word_length\": avg_word_len,\n",
        "            \"stop_word_ratio\": stop_word_ratio,\n",
        "            \"bigrams\": bigrams,\n",
        "            \"co_occurrences\": co_occurrences,\n",
        "            \"lexical_diversity\": ttr\n",
        "        }\n",
        "\n",
        "    def tfidf_keywords(self, top_n=10):\n",
        "        \"\"\"\n",
        "        Extract the top keywords from the text using TF-IDF scoring.\n",
        "\n",
        "        Args:\n",
        "            top_n (int, optional): Number of top keywords to return. Default is 10.\n",
        "\n",
        "        Returns:\n",
        "            list of tuples: Top (keyword, score) pairs sorted by score.\n",
        "        \"\"\"\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = vectorizer.fit_transform([self.text])\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "        keyword_scores = sorted(zip(feature_names, tfidf_scores), key=lambda x: x[1], reverse=True)\n",
        "        return keyword_scores[:top_n]\n",
        "\n",
        "    def avg_word_length(self):\n",
        "        \"\"\"\n",
        "        Calculate the average length of alphanumeric words in the text.\n",
        "\n",
        "        Returns:\n",
        "            float: Average word length in characters. Returns 0 if no valid words exist.\n",
        "        \"\"\"\n",
        "        word_lengths = [len(word) for word in self.words if word.isalnum()]\n",
        "        return sum(word_lengths) / len(word_lengths) if word_lengths else 0\n",
        "\n",
        "    def stop_word_ratio(self):\n",
        "        \"\"\"\n",
        "        Compute the proportion of stop words in the text.\n",
        "\n",
        "        Returns:\n",
        "            float: Ratio of stop words to total words. Returns 0 if no words exist.\n",
        "        \"\"\"\n",
        "        stop_word_count = len([word for word in self.words if word in self.stop_words])\n",
        "        total_words = len(self.words)\n",
        "        return stop_word_count / total_words if total_words > 0 else 0\n",
        "\n",
        "    def lexical_diversity(self):\n",
        "        \"\"\"\n",
        "        Calculate lexical diversity using the Type-Token Ratio (TTR).\n",
        "\n",
        "        Returns:\n",
        "            float: Unique words / total words. Returns 0 if total words is 0.\n",
        "        \"\"\"\n",
        "        unique_words = len(set(self.words))\n",
        "        total_words = len(self.words)\n",
        "        return unique_words / total_words if total_words > 0 else 0\n",
        "\n",
        "    def keyword_co_occurrence(self, top_n=5):\n",
        "        \"\"\"\n",
        "        Identify frequently co-occurring top keywords within the same sentence.\n",
        "\n",
        "        Args:\n",
        "            top_n (int, optional): Number of top keywords to consider. Default is 5.\n",
        "\n",
        "        Returns:\n",
        "            list of tuples: Pairs of keywords and their co-occurrence count.\n",
        "        \"\"\"\n",
        "        filtered_words = [word for word in self.words if word.isalnum() and word not in self.stop_words]\n",
        "        freq_dist = FreqDist(filtered_words)\n",
        "        top_keywords = [word for word, freq in freq_dist.most_common(top_n)]\n",
        "        co_occurrences = {}\n",
        "        sentences = sent_tokenize(self.text)\n",
        "\n",
        "        for sent in sentences:\n",
        "            sent_words = set(word_tokenize(sent.lower()))\n",
        "            for i, kw1 in enumerate(top_keywords):\n",
        "                if kw1 in sent_words:\n",
        "                    for kw2 in top_keywords[i+1:]:\n",
        "                        if kw2 in sent_words:\n",
        "                            pair = tuple(sorted([kw1, kw2]))\n",
        "                            co_occurrences[pair] = co_occurrences.get(pair, 0) + 1\n",
        "\n",
        "        return sorted(co_occurrences.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "    def ngram_analysis(self, n=2, top_n=5):\n",
        "        \"\"\"\n",
        "        Find the most frequent n-grams in the text (excluding stopwords and punctuation).\n",
        "\n",
        "        Args:\n",
        "            n (int, optional): Size of the n-gram (e.g., 2 = bigram). Default is 2.\n",
        "            top_n (int, optional): Number of top n-grams to return. Default is 5.\n",
        "\n",
        "        Returns:\n",
        "            list of tuples: Top (ngram, frequency) pairs sorted by frequency.\n",
        "        \"\"\"\n",
        "        words = [word for word in self.words if word.isalnum() and word not in self.stop_words]\n",
        "        ngrams = list(nltk.ngrams(words, n))\n",
        "        ngram_freq = FreqDist(ngrams)\n",
        "        return ngram_freq.most_common(top_n)\n"
      ],
      "metadata": {
        "id": "ec0UVPGo3Gg5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceLevelAnalysis:\n",
        "    \"\"\"\n",
        "    A class to perform sentence-level analysis on textual data.\n",
        "\n",
        "    It provides:\n",
        "    - Sentence count and length statistics\n",
        "    - Average sentence length\n",
        "    - Sentence-level sentiment analysis using TextBlob\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text):\n",
        "        \"\"\"\n",
        "        Initialize the SentenceLevelAnalysis with input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The raw input text to analyze at the sentence level.\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def sentence_statstics(self):\n",
        "        \"\"\"\n",
        "        Perform core sentence-level statistics on the text.\n",
        "\n",
        "        This includes:\n",
        "        - Token-based sentence length distribution\n",
        "        - Total sentence count\n",
        "        - Average sentence length\n",
        "        - Per-sentence sentiment polarity\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary with the following keys:\n",
        "                - sentence_lengths (list of int): Number of tokens per sentence.\n",
        "                - sentence_count (int): Total number of sentences.\n",
        "                - avg_sentence_length (float): Average number of tokens per sentence.\n",
        "                - sentence_sentiments (list of float): Sentiment polarity scores per sentence.\n",
        "        \"\"\"\n",
        "        doc = self.nlp(self.text)\n",
        "        self.sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "        sentence_lengths = [len(word_tokenize(sent)) for sent in self.sentences]\n",
        "        sentence_count = len(self.sentences)\n",
        "        word_count = len(word_tokenize(self.text))\n",
        "        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
        "        sentence_sentiments = self.sentence_sentiment_analysis()\n",
        "\n",
        "        return {\n",
        "            \"sentence_lengths\": sentence_lengths,\n",
        "            \"sentence_count\": sentence_count,\n",
        "            \"avg_sentence_length\": avg_sentence_length,\n",
        "            \"sentence_sentiments\": sentence_sentiments\n",
        "        }\n",
        "\n",
        "    def sentence_sentiment_analysis(self):\n",
        "        \"\"\"\n",
        "        Analyze sentiment polarity for each sentence using TextBlob.\n",
        "\n",
        "        This method processes the first 50 sentences (for efficiency and visual clarity)\n",
        "        and computes a polarity score between -1 (negative) and 1 (positive) for each.\n",
        "\n",
        "        Returns:\n",
        "            list of float: Sentiment polarity scores for up to 50 sentences.\n",
        "        \"\"\"\n",
        "        sentiments = []\n",
        "        for sent in self.sentences[:50]:\n",
        "            blob = TextBlob(sent)\n",
        "            sentiments.append(blob.sentiment.polarity)\n",
        "        return sentiments\n"
      ],
      "metadata": {
        "id": "DnthVpLy3KPV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParagraphLevelAnalysis:\n",
        "    \"\"\"\n",
        "    A class to perform paragraph-level analysis on a given text.\n",
        "\n",
        "    It includes:\n",
        "    - Paragraph count\n",
        "    - Sentence count per paragraph\n",
        "    - Paragraph length distribution\n",
        "    - Average number of sentences per paragraph\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text):\n",
        "        \"\"\"\n",
        "        Initialize the ParagraphLevelAnalysis with the input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input document or block of text to analyze.\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        self.sentences = sent_tokenize(self.text)\n",
        "\n",
        "    def paragraph_statstics(self):\n",
        "        \"\"\"\n",
        "        Compute paragraph-level statistics, including length distribution\n",
        "        and average number of sentences per paragraph.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing:\n",
        "                - paragraph_lengths (list of int): Number of sentences in each paragraph.\n",
        "                - paragraph_count (int): Total number of paragraphs.\n",
        "                - avg_sentences_per_paragraph (float): Average sentences per paragraph.\n",
        "        \"\"\"\n",
        "        paragraphs = [p.strip() for p in self.text.split('\\n') if p.strip()]\n",
        "        paragraph_lengths = [len(sent_tokenize(p)) for p in paragraphs]\n",
        "        paragraph_count, avg_sentences_per_paragraph = self.paragraph_stats()\n",
        "\n",
        "        return {\n",
        "            \"paragraph_lengths\": paragraph_lengths,\n",
        "            \"paragraph_count\": paragraph_count,\n",
        "            \"avg_sentences_per_paragraph\": avg_sentences_per_paragraph\n",
        "        }\n",
        "\n",
        "    def paragraph_stats(self):\n",
        "        \"\"\"\n",
        "        Calculate core statistics for paragraphs.\n",
        "\n",
        "        A paragraph is defined as a non-empty block of text separated by newlines.\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - paragraph_count (int): Number of paragraphs.\n",
        "                - avg_sentences (float): Average number of sentences per paragraph.\n",
        "        \"\"\"\n",
        "        paragraphs = [p.strip() for p in self.text.split('\\n') if p.strip()]\n",
        "        paragraph_count = len(paragraphs)\n",
        "        sentences_per_paragraph = [len(sent_tokenize(p)) for p in paragraphs]\n",
        "        avg_sentences = sum(sentences_per_paragraph) / paragraph_count if paragraph_count > 0 else 0\n",
        "        return paragraph_count, avg_sentences\n"
      ],
      "metadata": {
        "id": "CzZ-7Wxi3O1x"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocLevelAnalysis:\n",
        "    \"\"\"\n",
        "    A class to perform high-level document analysis.\n",
        "\n",
        "    This includes:\n",
        "    - Word count\n",
        "    - Readability scores (Flesch Reading Ease & Flesch-Kincaid Grade)\n",
        "    - Named entity recognition (NER)\n",
        "    - Overall document sentiment\n",
        "    - Punctuation frequency\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text):\n",
        "        \"\"\"\n",
        "        Initialize the DocLevelAnalysis with the given document text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The full document content as a string.\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        self.words = word_tokenize(self.text)\n",
        "        self.sentences = sent_tokenize(self.text)\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def doc_statstics(self):\n",
        "        \"\"\"\n",
        "        Compute key document-level statistics.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing:\n",
        "                - word_count (int): Total number of words.\n",
        "                - readability_ease (float): Flesch Reading Ease score.\n",
        "                - readability_grade (float): Flesch-Kincaid Grade Level.\n",
        "                - named_entities (dict): Named entity type counts.\n",
        "                - sentiment (dict): Overall sentiment score and label.\n",
        "                - punctuation_freq (list of tuples): Punctuation and their frequencies.\n",
        "        \"\"\"\n",
        "        word_count = len(self.words)\n",
        "        readability_ease = flesch_reading_ease(self.text)\n",
        "        readability_grade = flesch_kincaid_grade(self.text)\n",
        "        entity_counts = self.named_entity_distribution()\n",
        "        sentiment = self.sentiment_analysis()\n",
        "        punctuation_freq = self.punctuation_frequency()\n",
        "\n",
        "        return {\n",
        "            \"word_count\": word_count,\n",
        "            \"readability_ease\": readability_ease,\n",
        "            \"readability_grade\": readability_grade,\n",
        "            \"named_entities\": entity_counts,\n",
        "            \"sentiment\": sentiment,\n",
        "            \"punctuation_freq\": punctuation_freq\n",
        "        }\n",
        "\n",
        "    def sentiment_analysis(self):\n",
        "        \"\"\"\n",
        "        Compute overall sentiment polarity of the entire document using TextBlob.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary with:\n",
        "                - 'score' (float): Sentiment polarity score in range [-1, 1].\n",
        "                - 'label' (str): Sentiment label: \"positive\", \"neutral\", or \"negative\".\n",
        "        \"\"\"\n",
        "        blob = TextBlob(self.text)\n",
        "        sentiment = blob.sentiment.polarity\n",
        "        label = \"positive\" if sentiment > 0.1 else \"negative\" if sentiment < -0.1 else \"neutral\"\n",
        "        return {\"score\": sentiment, \"label\": label}\n",
        "\n",
        "    def named_entity_distribution(self):\n",
        "        \"\"\"\n",
        "        Identify and count named entities using spaCy's NER pipeline.\n",
        "\n",
        "        Returns:\n",
        "            dict: Mapping of entity types (e.g., 'PERSON', 'ORG') to unique counts.\n",
        "                  Example: {'PERSON': 3, 'ORG': 2, 'DATE': 5}\n",
        "        \"\"\"\n",
        "        doc = self.nlp(self.text)\n",
        "        entity_info = {}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            label = ent.label_\n",
        "            text = ent.text.strip()\n",
        "            if label not in entity_info:\n",
        "                entity_info[label] = set()\n",
        "            entity_info[label].add(text)\n",
        "\n",
        "        return {label: len(entities) for label, entities in entity_info.items()}\n",
        "\n",
        "    def punctuation_frequency(self):\n",
        "        \"\"\"\n",
        "        Count frequency of common punctuation marks in the document.\n",
        "\n",
        "        Target punctuation marks: . , ! ? ; :\n",
        "\n",
        "        Returns:\n",
        "            list of tuples: Punctuation and their frequencies, sorted in descending order.\n",
        "                            Example: [('.', 14), (',', 10), ('!', 3)]\n",
        "        \"\"\"\n",
        "        punctuation = re.findall(r'[.,!?;:]', self.text)\n",
        "        punctuation_freq = FreqDist(punctuation)\n",
        "        return punctuation_freq.most_common()\n"
      ],
      "metadata": {
        "id": "_sYkXMou3R1G"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_key_insights(word_result, sent_result, para_result, doc_result):\n",
        "    \"\"\"\n",
        "    Generate an executive summary of the documentâ€™s key linguistic and structural insights.\n",
        "\n",
        "    This summary provides a concise overview of:\n",
        "        - Overall sentiment polarity (positive, neutral, negative)\n",
        "        - Readability difficulty based on Flesch Reading Ease\n",
        "        - Named entity type diversity from document-level analysis\n",
        "        - Lexical diversity measured by the Type-Token Ratio (TTR)\n",
        "\n",
        "    Args:\n",
        "        word_result (dict): Output from WordLevelAnalysis.word_statstics().\n",
        "        sent_result (dict): Output from SentenceLevelAnalysis.sentence_statstics().\n",
        "        para_result (dict): Output from ParagraphLevelAnalysis.paragraph_statstics().\n",
        "        doc_result (dict): Output from DocLevelAnalysis.doc_statstics().\n",
        "\n",
        "    Returns:\n",
        "        str: A grammatically structured sentence summarizing tone, readability, entity richness,\n",
        "             and vocabulary variety of the input document.\n",
        "    \"\"\"\n",
        "    insights = []\n",
        "\n",
        "    # Sentiment insight\n",
        "    sentiment_label = doc_result[\"sentiment\"][\"label\"]\n",
        "    insights.append(f\"This document has a {sentiment_label} tone (score: {doc_result['sentiment']['score']:.2f})\")\n",
        "\n",
        "    # Readability insight\n",
        "    readability = \"easy\" if doc_result[\"readability_ease\"] > 60 else \"moderate\" if doc_result[\"readability_ease\"] > 30 else \"difficult\"\n",
        "    insights.append(f\"readability is {readability} (Flesch Reading Ease: {doc_result['readability_ease']:.2f})\")\n",
        "\n",
        "    # Named entity diversity\n",
        "    entity_diversity = len(doc_result[\"named_entities\"])\n",
        "    entity_insight = \"low\" if entity_diversity < 5 else \"high\" if entity_diversity > 10 else \"moderate\"\n",
        "    insights.append(f\"named entity diversity is {entity_insight} ({entity_diversity} entity types)\")\n",
        "\n",
        "    # Lexical diversity\n",
        "    lexical_insight = \"high\" if word_result[\"lexical_diversity\"] > 0.5 else \"low\" if word_result[\"lexical_diversity\"] < 0.3 else \"moderate\"\n",
        "    insights.append(f\"lexical diversity is {lexical_insight} (TTR: {word_result['lexical_diversity']:.2f})\")\n",
        "\n",
        "    return \", \".join(insights) + \".\"\n"
      ],
      "metadata": {
        "id": "BagBBhaJXQ51"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocVisulaization:\n",
        "    \"\"\"\n",
        "    A utility class for generating visualizations (word cloud, histograms, bar charts, heatmaps)\n",
        "    from document analysis results and saving them as image files in a specified directory.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, report_dir):\n",
        "        \"\"\"\n",
        "        Initialize the visualization class with a target directory for saving plots.\n",
        "\n",
        "        Args:\n",
        "            report_dir (str): Path to the directory where visualization images will be saved.\n",
        "        \"\"\"\n",
        "        self.report_dir = report_dir\n",
        "\n",
        "    def generate_word_cloud(self, words):\n",
        "        \"\"\"\n",
        "        Generate and save a word cloud image from a list of words.\n",
        "\n",
        "        Args:\n",
        "            words (list of str): Words to include in the word cloud.\n",
        "\n",
        "        Returns:\n",
        "            str: Path to the saved word cloud image file.\n",
        "        \"\"\"\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stop_words).generate(' '.join(words))\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        image_path = os.path.join(self.report_dir, \"wordcloud.png\")\n",
        "        plt.savefig(image_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        return image_path\n",
        "\n",
        "    def generate_histogram(self, data, title, xlabel, ylabel, filename):\n",
        "        \"\"\"\n",
        "        Generate and save a histogram from numerical data.\n",
        "\n",
        "        Args:\n",
        "            data (list or array-like): Numerical values to plot.\n",
        "            title (str): Plot title.\n",
        "            xlabel (str): Label for the X-axis.\n",
        "            ylabel (str): Label for the Y-axis.\n",
        "            filename (str): Output file name (e.g., 'histogram.png').\n",
        "\n",
        "        Returns:\n",
        "            str: Path to the saved histogram image file.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.hist(data, bins=20, color='skyblue', edgecolor='black')\n",
        "        plt.title(title)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        image_path = os.path.join(self.report_dir, filename)\n",
        "        plt.savefig(image_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        return image_path\n",
        "\n",
        "    def generate_bar_chart(self, labels, values, title, xlabel, ylabel, filename):\n",
        "        \"\"\"\n",
        "        Generate and save a bar chart with the provided labels and values.\n",
        "\n",
        "        Args:\n",
        "            labels (list of str): X-axis categories.\n",
        "            values (list of float): Heights of the bars corresponding to each label.\n",
        "            title (str): Plot title.\n",
        "            xlabel (str): Label for the X-axis.\n",
        "            ylabel (str): Label for the Y-axis.\n",
        "            filename (str): Output file name (e.g., 'barchart.png').\n",
        "\n",
        "        Returns:\n",
        "            str: Path to the saved bar chart image file.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.bar(labels, values, color='lightgreen')\n",
        "        plt.title(title)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.xticks(rotation=45)\n",
        "        image_path = os.path.join(self.report_dir, filename)\n",
        "        plt.savefig(image_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        return image_path\n",
        "\n",
        "    def generate_heatmap(self, data, title, filename):\n",
        "        \"\"\"\n",
        "        Generate and save a heatmap from a 2D matrix of sentiment scores.\n",
        "\n",
        "        Args:\n",
        "            data (2D list or array): Sentiment scores; each row can represent a different metric.\n",
        "            title (str): Plot title.\n",
        "            filename (str): Output file name (e.g., 'heatmap.png').\n",
        "\n",
        "        Returns:\n",
        "            str: Path to the saved heatmap image file.\n",
        "        \"\"\"\n",
        "        data = np.array(data).reshape(-1, len(data))\n",
        "        plt.figure(figsize=(10, 2))\n",
        "        sns.heatmap(data, cmap='RdYlGn', vmin=-1, vmax=1, annot=True, fmt=\".2f\",\n",
        "                    cbar=False, annot_kws={\"rotation\": 90})\n",
        "        plt.title(title)\n",
        "        plt.xlabel(\"Sentence Index\")\n",
        "        plt.ylabel(\"Sentiment\")\n",
        "        plt.xticks(np.arange(len(data[0])) + 0.5, np.arange(len(data[0])))\n",
        "        image_path = os.path.join(self.report_dir, filename)\n",
        "        plt.savefig(image_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        return image_path\n"
      ],
      "metadata": {
        "id": "X_MJy3Qnwo1Y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocWriter:\n",
        "    \"\"\"\n",
        "    A class responsible for generating document analysis reports with optional visualizations\n",
        "    and packaging the output into a downloadable ZIP archive.\n",
        "\n",
        "    Features:\n",
        "    - Generates detailed reports in Markdown, HTML, or PDF format\n",
        "    - Includes insights from multiple levels of analysis (word, sentence, paragraph, document)\n",
        "    - Embeds visualizations such as word clouds, histograms, bar charts, and heatmaps\n",
        "    - Packages report and visualizations into a ZIP archive for download\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, report_dir, word_result, sent_result, para_result, doc_result):\n",
        "        \"\"\"\n",
        "        Initialize the report writer with analysis results and target output directory.\n",
        "\n",
        "        Args:\n",
        "            report_dir (str): Directory to save the report and visualizations.\n",
        "            word_result (dict): Output from WordLevelAnalysis.\n",
        "            sent_result (dict): Output from SentenceLevelAnalysis.\n",
        "            para_result (dict): Output from ParagraphLevelAnalysis.\n",
        "            doc_result (dict): Output from DocLevelAnalysis.\n",
        "        \"\"\"\n",
        "        self.report_dir = report_dir\n",
        "        self.word_result = word_result\n",
        "        self.sent_result = sent_result\n",
        "        self.para_result = para_result\n",
        "        self.doc_result = doc_result\n",
        "\n",
        "    def _create_zip(self, report_path, image_paths):\n",
        "        \"\"\"\n",
        "        Create a ZIP archive containing the report and all generated image files.\n",
        "\n",
        "        Args:\n",
        "            report_path (str): File path to the main report (e.g., .md, .pdf, or .html).\n",
        "            image_paths (list): List of image file paths to include in the archive.\n",
        "\n",
        "        Returns:\n",
        "            str: Path to the generated ZIP archive.\n",
        "        \"\"\"\n",
        "        zip_path = os.path.join(self.report_dir, \"docstats_report.zip\")\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            zipf.write(report_path, os.path.basename(report_path))\n",
        "            for image_path in image_paths:\n",
        "                zipf.write(image_path, os.path.join(\"images\", os.path.basename(image_path)))\n",
        "        return zip_path\n",
        "\n",
        "    def _image_to_base64(self, image_path):\n",
        "        \"\"\"\n",
        "        Convert an image to a base64-encoded string for inline embedding in Markdown/HTML.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the image file.\n",
        "\n",
        "        Returns:\n",
        "            str: Base64-encoded data URI string.\n",
        "        \"\"\"\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            encoded = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "        return f\"data:image/png;base64,{encoded}\"\n",
        "\n",
        "    def generate_report(self, stats_to_include=\"all\", include_visualizations=None, format=\"markdown\"):\n",
        "        \"\"\"\n",
        "        Generate a comprehensive report of document statistics and visualizations.\n",
        "\n",
        "        The report can be exported in Markdown, HTML, or PDF format and includes:\n",
        "        - Executive summary of sentiment, readability, and lexical features\n",
        "        - Analysis sections based on selected stats\n",
        "        - Embedded images for visualization\n",
        "        - Packaged ZIP archive for download\n",
        "\n",
        "        Args:\n",
        "            stats_to_include (str or list, optional): \"all\" or a list of sections to include. Options:\n",
        "                [\"basic\", \"readability\", \"linguistic\", \"pos\", \"named_entities\",\n",
        "                 \"keyword_analysis\", \"punctuation\", \"visualizations\"]\n",
        "            include_visualizations (list, optional): Visualizations to generate. Options:\n",
        "                [\"wordcloud\", \"sentence_histogram\", \"pos_bar\", \"sentiment_heatmap\", \"paragraph_histogram\"]\n",
        "            format (str, optional): Output format: \"markdown\", \"html\", or \"pdf\". Default is \"markdown\".\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - report_path (str): Path to the saved report file.\n",
        "                - zip_path (str): Path to the ZIP archive containing the report and images.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If an unsupported report format is provided.\n",
        "        \"\"\"\n",
        "        visualize = DocVisulaization(self.report_dir)\n",
        "\n",
        "        if stats_to_include == \"all\":\n",
        "            stats_to_include = [\n",
        "                \"basic\", \"readability\", \"linguistic\", \"pos\", \"named_entities\",\n",
        "                \"keyword_analysis\", \"punctuation\", \"visualizations\"\n",
        "            ]\n",
        "\n",
        "        if include_visualizations is None:\n",
        "            include_visualizations = [\n",
        "                \"wordcloud\", \"sentence_histogram\", \"pos_bar\",\n",
        "                \"sentiment_heatmap\", \"paragraph_histogram\"\n",
        "            ]\n",
        "\n",
        "        # Generate visualizations\n",
        "        visualizations = {}\n",
        "        image_paths = []\n",
        "\n",
        "        if \"wordcloud\" in include_visualizations:\n",
        "            visualizations[\"wordcloud\"] = visualize.generate_word_cloud(self.word_result[\"filtered_words\"])\n",
        "            image_paths.append(visualizations[\"wordcloud\"])\n",
        "\n",
        "        if \"sentence_histogram\" in include_visualizations:\n",
        "            visualizations[\"sentence_histogram\"] = visualize.generate_histogram(\n",
        "                self.sent_result[\"sentence_lengths\"], \"Sentence Length Distribution\",\n",
        "                \"Sentence Length (words)\", \"Frequency\", \"sentence_length_histogram.png\"\n",
        "            )\n",
        "            image_paths.append(visualizations[\"sentence_histogram\"])\n",
        "\n",
        "        if \"pos_bar\" in include_visualizations:\n",
        "            visualizations[\"pos_bar\"] = visualize.generate_bar_chart(\n",
        "                [tag for tag, _ in self.word_result[\"pos_tags\"]],\n",
        "                [freq for _, freq in self.word_result[\"pos_tags\"]],\n",
        "                \"Top 5 Part-of-Speech Tags\", \"POS Tag\", \"Frequency\", \"pos_distribution.png\"\n",
        "            )\n",
        "            image_paths.append(visualizations[\"pos_bar\"])\n",
        "\n",
        "        if \"sentiment_heatmap\" in include_visualizations:\n",
        "            visualizations[\"sentiment_heatmap\"] = visualize.generate_heatmap(\n",
        "                self.sent_result[\"sentence_sentiments\"],\n",
        "                \"Sentence-Level Sentiment Heatmap\",\n",
        "                \"sentiment_heatmap.png\"\n",
        "            )\n",
        "            image_paths.append(visualizations[\"sentiment_heatmap\"])\n",
        "\n",
        "        if \"paragraph_histogram\" in include_visualizations:\n",
        "            visualizations[\"paragraph_histogram\"] = visualize.generate_histogram(\n",
        "                self.para_result[\"paragraph_lengths\"], \"Paragraph Length Distribution\",\n",
        "                \"Paragraph Length (sentences)\", \"Frequency\", \"paragraph_length_histogram.png\"\n",
        "            )\n",
        "            image_paths.append(visualizations[\"paragraph_histogram\"])\n",
        "\n",
        "        # Convert images to base64 for embedding\n",
        "        visualizations_base64 = {\n",
        "            k: self._image_to_base64(v) for k, v in visualizations.items()\n",
        "        }\n",
        "\n",
        "        # Start building the report content\n",
        "        report = \"# Document Statistics Report\\n\\n## Table of Contents\\n\"\n",
        "        if \"basic\" in stats_to_include:\n",
        "            report += \"- [Basic Statistics](#basic-statistics)\\n\"\n",
        "        if \"readability\" in stats_to_include:\n",
        "            report += \"- [Readability Scores](#readability-scores)\\n\"\n",
        "        if \"linguistic\" in stats_to_include:\n",
        "            report += \"- [Linguistic Analysis](#linguistic-analysis)\\n\"\n",
        "        if \"pos\" in stats_to_include:\n",
        "            report += \"- [Part-of-Speech Distribution](#part-of-speech-distribution)\\n\"\n",
        "        if \"named_entities\" in stats_to_include:\n",
        "            report += \"- [Named Entity Distribution](#named-entity-distribution)\\n\"\n",
        "        if \"keyword_analysis\" in stats_to_include:\n",
        "            report += \"- [Keyword Analysis](#keyword-analysis)\\n\"\n",
        "        if \"punctuation\" in stats_to_include:\n",
        "            report += \"- [Punctuation Frequency](#punctuation-frequency)\\n\"\n",
        "        if \"visualizations\" in stats_to_include:\n",
        "            report += \"- [Visualizations](#visualizations)\\n\"\n",
        "\n",
        "        # Executive Summary\n",
        "        report += \"\\n## Executive Summary\\n\"\n",
        "        report += generate_key_insights(self.word_result, self.sent_result, self.para_result, self.doc_result) + \"\\n\\n\"\n",
        "        report += f\"**File:** {os.path.basename(self.report_dir)}\\n\\n\"\n",
        "\n",
        "        # Basic Stats\n",
        "        if \"basic\" in stats_to_include:\n",
        "            report += \"## Basic Statistics\\n\"\n",
        "            report += f\"- Word Count: {self.doc_result['word_count']}\\n\"\n",
        "            report += f\"- Sentence Count: {self.sent_result['sentence_count']}\\n\"\n",
        "            report += f\"- Avg Sentence Length: {self.sent_result['avg_sentence_length']:.2f} words\\n\"\n",
        "            report += f\"- Avg Word Length: {self.word_result['avg_word_length']:.2f} characters\\n\"\n",
        "            report += f\"- Paragraph Count: {self.para_result['paragraph_count']}\\n\"\n",
        "            report += f\"- Avg Sentences per Paragraph: {self.para_result['avg_sentences_per_paragraph']:.2f}\\n\\n\"\n",
        "\n",
        "        # Readability\n",
        "        if \"readability\" in stats_to_include:\n",
        "            report += \"## Readability Scores\\n\"\n",
        "            report += f\"- Flesch Reading Ease: {self.doc_result['readability_ease']:.2f}\\n\"\n",
        "            report += f\"- Flesch-Kincaid Grade Level: {self.doc_result['readability_grade']:.2f}\\n\\n\"\n",
        "\n",
        "        # Linguistic\n",
        "        if \"linguistic\" in stats_to_include:\n",
        "            report += \"## Linguistic Analysis\\n\"\n",
        "            report += f\"- Lexical Diversity (TTR): {self.word_result['lexical_diversity']:.2f}\\n\"\n",
        "            report += f\"- Stop Word Ratio: {self.word_result['stop_word_ratio']:.2f}\\n\"\n",
        "            report += f\"- Sentiment Score: {self.doc_result['sentiment']['score']:.2f} ({self.doc_result['sentiment']['label']})\\n\\n\"\n",
        "\n",
        "        # POS\n",
        "        if \"pos\" in stats_to_include:\n",
        "            report += \"## Part-of-Speech Distribution\\n\"\n",
        "            for tag, freq in self.word_result[\"pos_tags\"]:\n",
        "                report += f\"- {tag}: {freq}\\n\"\n",
        "            if \"pos_bar\" in include_visualizations:\n",
        "                report += f\"\\n![POS Distribution]({visualizations_base64['pos_bar']})\\n\\n\"\n",
        "\n",
        "        # Named Entities\n",
        "        if \"named_entities\" in stats_to_include:\n",
        "            report += \"## Named Entity Distribution\\n\"\n",
        "            for label, count in self.doc_result[\"named_entities\"].items():\n",
        "                report += f\"- {label}: {count}\\n\"\n",
        "            report += \"\\n\"\n",
        "\n",
        "        # Keyword Analysis\n",
        "        if \"keyword_analysis\" in stats_to_include:\n",
        "            report += \"## Keyword Analysis\\n\"\n",
        "            report += \"### Top Keywords (TF-IDF)\\n\"\n",
        "            for word, score in self.word_result[\"top_keywords\"]:\n",
        "                report += f\"- {word}: {score:.2f}\\n\"\n",
        "            report += \"### Top Bigrams\\n\"\n",
        "            for bigram, freq in self.word_result[\"bigrams\"]:\n",
        "                report += f\"- {' '.join(bigram)}: {freq}\\n\"\n",
        "            report += \"### Keyword Co-Occurrence\\n\"\n",
        "            for (w1, w2), freq in self.word_result[\"co_occurrences\"]:\n",
        "                report += f\"- {w1} & {w2}: {freq} co-occurrences\\n\"\n",
        "            report += \"\\n\"\n",
        "\n",
        "        # Punctuation\n",
        "        if \"punctuation\" in stats_to_include:\n",
        "            report += \"## Punctuation Frequency\\n\"\n",
        "            for punc, freq in self.doc_result[\"punctuation_freq\"]:\n",
        "                report += f\"- {punc}: {freq}\\n\"\n",
        "            report += \"\\n\"\n",
        "\n",
        "        # Visualizations\n",
        "        if \"visualizations\" in stats_to_include:\n",
        "            report += \"## Visualizations\\n\"\n",
        "            for key in include_visualizations:\n",
        "                if key in visualizations_base64:\n",
        "                    report += f\"### {key.replace('_', ' ').title()}\\n\"\n",
        "                    report += f\"![{key}]({visualizations_base64[key]})\\n\\n\"\n",
        "\n",
        "        # Save Report\n",
        "        if format == \"markdown\":\n",
        "            report_path = os.path.join(self.report_dir, \"report.md\")\n",
        "            with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(report)\n",
        "        elif format == \"html\":\n",
        "            report_path = os.path.join(self.report_dir, \"report.html\")\n",
        "            with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(markdown(report))\n",
        "        elif format == \"pdf\":\n",
        "            report_path = os.path.join(self.report_dir, \"report.pdf\")\n",
        "            HTML(string=markdown(report)).write_pdf(report_path)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported format. Use 'markdown', 'html', or 'pdf'.\")\n",
        "\n",
        "        # Bundle into ZIP\n",
        "        zip_path = self._create_zip(report_path, image_paths)\n",
        "\n",
        "        return report_path, zip_path\n"
      ],
      "metadata": {
        "id": "Q2aA7KGnrWfH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def process_document(file, file_type, output_format):\n",
        "    logger.info(\"Starting document processing\")\n",
        "    if not file:\n",
        "        logger.error(\"No file uploaded\")\n",
        "        return \"Please upload a file.\", None, None\n",
        "\n",
        "    temp_file = None\n",
        "    try:\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=file_type, prefix=\"uploaded_\")\n",
        "\n",
        "        # Handle file input\n",
        "        if isinstance(file, bytes):\n",
        "            content = file\n",
        "        else:\n",
        "            with open(file.name, \"rb\") as f:\n",
        "                content = f.read()\n",
        "        with open(temp_file.name, \"wb\") as f:\n",
        "            f.write(content)\n",
        "        logger.info(f\"Saved uploaded file to: {temp_file.name}\")\n",
        "\n",
        "        # === Modular pipeline ===\n",
        "        read_obj = DocReader(temp_file.name)\n",
        "        text = read_obj.extract_text()\n",
        "\n",
        "        word_obj = WordLevelAnalysis(text)\n",
        "        word_result = word_obj.word_statstics()\n",
        "\n",
        "        sent_obj = SentenceLevelAnalysis(text)\n",
        "        sent_result = sent_obj.sentence_statstics()\n",
        "\n",
        "        para_obj = ParagraphLevelAnalysis(text)\n",
        "        para_result = para_obj.paragraph_statstics()\n",
        "\n",
        "        doc_obj = DocLevelAnalysis(text)\n",
        "        doc_result = doc_obj.doc_statstics()\n",
        "\n",
        "        write_obj = DocWriter(\"./\", word_result, sent_result, para_result, doc_result)\n",
        "        report_path, zip_path = write_obj.generate_report(format=output_format)\n",
        "\n",
        "        # Read report content\n",
        "        if output_format in [\"markdown\", \"html\"]:\n",
        "            with open(report_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                report_content = f.read()\n",
        "        else:\n",
        "            report_content = \"PDF generated. Download the ZIP file to view the report.\"\n",
        "\n",
        "        return report_content, report_path, zip_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error during processing\")\n",
        "        return f\"Error: {e}\", None, None\n",
        "\n",
        "    finally:\n",
        "        if temp_file and os.path.exists(temp_file.name):\n",
        "            os.remove(temp_file.name)\n",
        "            logger.info(f\"Deleted temp file: {temp_file.name}\")\n",
        "\n",
        "def launch_gradio_interface():\n",
        "    iface = gr.Interface(\n",
        "        fn=process_document,\n",
        "        inputs=[\n",
        "            gr.File(label=\"Upload Document\", file_types=[\".pdf\", \".docx\", \".txt\"]),\n",
        "            gr.Dropdown(choices=[\".pdf\", \".docx\", \".txt\"], label=\"File Type\", value=\".pdf\"),\n",
        "            gr.Dropdown(choices=[\"markdown\", \"html\", \"pdf\"], label=\"Output Format\", value=\"markdown\")\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Markdown(label=\"Report Preview\"),\n",
        "            gr.File(label=\"Download Report\"),\n",
        "            gr.File(label=\"Download ZIP Archive\")\n",
        "        ],\n",
        "        title=\"Document Statistics Analyzer\",\n",
        "        description=\"Upload a document and generate word, sentence, paragraph, and document-level insights.\"\n",
        "    )\n",
        "    iface.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    launch_gradio_interface()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "fqSWLa8WjMfu",
        "outputId": "3c92d81a-0208-4621-cea1-736586d7071b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://064215d29687a99b62.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://064215d29687a99b62.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AycktIp9ojrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}